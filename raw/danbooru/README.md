Note: the 2025-04-07 compressed files have the following sizes uncompressed

- tag_aliases.json: 17.3 MB

- tag_implications.json: 13.1 MB

- tags.json: 247.3 MB


others (for 2025-05-01 and maybe beyond):

- artists: 156.8 MB

- pools.json: 35.3 MB

- wiki_pages.json: 125.8 MB

- comments_1000pages.json: 453.8 MB

- notes_1000pages.json: 329.7 MB

- users: excluded because weird errors when nothing else errored idk

*if a file has 1000pages or so in it's name then it means that only up to 1000 pages were scraped* because danbooru doesnt allow more, 2025-05-01 is probably the only folder where this shows up. For full data look for an official dump or something i guess

These files can be decompressed with 7z/NanaZip and possibly almost everything that supports PPMd compression method.
If you have 7z or anything that installs 7z to PATH installed you can also use these python scripts (which i used to compress the files) https://github.com/DraconicDragon/danbooru-e621-tag-list-processor/tree/feat/raw-mode/extras/misc